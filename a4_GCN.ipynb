{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImLCXm8IsSS2"
   },
   "source": [
    "# Download the Cora data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xRN47p1SKRgP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-08 13:22:05--  https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
      "wget: Cannot read /home/maryamebr/.netrc (Permission denied).\n",
      "Resolving linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)... 128.114.47.74\n",
      "Connecting to linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)|128.114.47.74|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 168052 (164K) [application/x-gzip]\n",
      "Saving to: ‘cora.tgz’\n",
      "\n",
      "cora.tgz            100%[===================>] 164.11K   707KB/s    in 0.2s    \n",
      "\n",
      "2022-04-08 13:22:09 (707 KB/s) - ‘cora.tgz’ saved [168052/168052]\n",
      "\n",
      "cora/\n",
      "cora/README\n",
      "cora/cora.cites\n",
      "cora/cora.content\n"
     ]
    }
   ],
   "source": [
    "! wget https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
    "! tar -zxvf cora.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXIYzURA4OKg"
   },
   "source": [
    "# import modules and set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uJQYMX02_z0M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "seed = 0\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgOv1h7YsK-5"
   },
   "source": [
    "# Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kXPHN61i9keB"
   },
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    # The classes must be sorted before encoding to enable static class encoding.\n",
    "    # In other words, make sure the first class always maps to index 0.\n",
    "    classes = sorted(list(set(labels)))\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "########## ???? **** change this\n",
    "# def load_data(path=\"/content/cora/\", dataset=\"cora\", training_samples=140):\n",
    "def load_data(path=\"cora/\", dataset=\"cora\", training_samples=140):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    adj = normalize_adj(adj)\n",
    "\n",
    "    # Random indexes\n",
    "    idx_rand = torch.randperm(len(labels))\n",
    "    # Nodes for training\n",
    "    idx_train = idx_rand[:training_samples]\n",
    "    # Nodes for validation\n",
    "    idx_val= idx_rand[training_samples:]\n",
    "\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"symmetric normalization\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzCZVd1JsbHr"
   },
   "source": [
    "## check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KlsKjMKx8_b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mxrv21rLnpiZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1667, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.2000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2500]])\n",
      "torch.Size([2708, 2708])\n"
     ]
    }
   ],
   "source": [
    "print(adj)\n",
    "print(adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lWrDf0iWnpqV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([2708, 1433])\n"
     ]
    }
   ],
   "source": [
    "print(features)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TUkt2JJdsuA2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 5, 4,  ..., 1, 0, 2])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "2708\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(labels.unique())\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iGP18jNAs1Gp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "2568\n"
     ]
    }
   ],
   "source": [
    "print(len(idx_train))\n",
    "print(len(idx_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHqIcfH-vIic"
   },
   "source": [
    "# Vanilla GCN for node classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f48tylWyjLPE"
   },
   "source": [
    "## Define Graph Convolution layer (Your Task)\n",
    "\n",
    "This module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$.\n",
    "1.   perform initial transformation: $\\mathbf{s} = \\mathbf{W} \\times \\mathbf{h} ^{(l)}$\n",
    "2.   multiply $\\mathbf{s}$ by normalized adjacency matrix: $\\mathbf{h'} = \\mathbf{A} \\times \\mathbf{s}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M-fU8L7f41VZ"
   },
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    A Graph Convolution Layer (GCN)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        \"\"\"\n",
    "        * `in_features`, $F$, is the number of input features per node\n",
    "        * `out_features`, $F'$, is the number of output features per node\n",
    "        * `bias`, whether to include the bias term in the linear layer. Default=True\n",
    "        \"\"\"\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        # TODO: initialize the weight W that maps the input feature (dim F ) to output feature (dim F')\n",
    "        # hint: use nn.Linear()\n",
    "        ############ Your code here ###################################\n",
    "        self.W = nn.Linear(in_features=in_features, out_features=out_features, bias=bias)\n",
    "\n",
    "        ###############################################################\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # TODO: transform input feature to output (don't forget to use the adjacency matrix \n",
    "        # to sum over neighbouring nodes )\n",
    "        # hint: use the linear layer you declared above. \n",
    "        # hint: you can use torch.spmm() sparse matrix multiplication to handle the \n",
    "        #       adjacency matrix\n",
    "        ############ Your code here ###################################\n",
    "        out = self.W(input)\n",
    "        return torch.spmm(adj, out)\n",
    "\n",
    "        ###############################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxBELCxkjF6F"
   },
   "source": [
    "## Define GCN (Your Task)\n",
    "\n",
    "you will implement a two-layer GCN with ReLU activation function and Dropout after the first Conv layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HtVr2cN8jD5t"
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    '''\n",
    "    A two-layer GCN\n",
    "    '''\n",
    "    def __init__(self, nfeat, n_hidden, n_classes, dropout, bias=True):\n",
    "        \"\"\"\n",
    "        * `nfeat`, is the number of input features per node of the first layer\n",
    "        * `n_hidden`, number of hidden units\n",
    "        * `n_classes`, total number of classes for classification\n",
    "        * `dropout`, the dropout ratio\n",
    "        * `bias`, whether to include the bias term in the linear layer. Default=True\n",
    "        \"\"\"\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "        # TODO: Initialization\n",
    "        # (1) 2 GraphConvolution() layers. \n",
    "        # (2) 1 Dropout layer\n",
    "        # (3) 1 activation function: ReLU()\n",
    "        ############ Your code here ###################################\n",
    "        self.conv1 = GraphConvolution(nfeat, n_hidden, bias=bias)\n",
    "        self.conv2 = GraphConvolution(n_hidden, n_classes, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        ###############################################################\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # TODO: the input will pass through the first graph convolution layer, \n",
    "        # the activation function, the dropout layer, then the second graph \n",
    "        # convolution layer. No activation function for the \n",
    "        # last layer. Return the logits. \n",
    "        ############ Your code here ##################################\n",
    "        x = self.conv1(x, adj)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, adj)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "        ###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX1d9F1G508r"
   },
   "source": [
    "## define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HyhqJ39OCzNN"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXsdid6C5K1c"
   },
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bjlYeoFPFAWm"
   },
   "outputs": [],
   "source": [
    "args = {\"training_samples\": 140,\n",
    "        \"epochs\": 100,\n",
    "        \"lr\": 0.01,\n",
    "        \"weight_decay\": 5e-4,\n",
    "        \"hidden\": 16,\n",
    "        \"dropout\": 0.5,\n",
    "        \"bias\": True, \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Qbx0uc-9G5vs"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = criterion(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    loss_val = criterion(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = criterion(output[idx_val], labels[idx_val])\n",
    "    acc_test = accuracy(output[idx_val], labels[idx_val])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TjNiui83FYBr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "model = GCN(nfeat=features.shape[1],\n",
    "            n_hidden=args[\"hidden\"],\n",
    "            n_classes=labels.max().item() + 1,\n",
    "            dropout=args[\"dropout\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
    "\n",
    "\n",
    "adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n",
    "adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W6tqqj16iz-"
   },
   "source": [
    "## training Vanilla GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "WSjUYJPSlnOU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9470 acc_train: 0.0571 loss_val: 1.9335 acc_val: 0.0814 time: 0.0036s\n",
      "Epoch: 0002 loss_train: 1.9401 acc_train: 0.0571 loss_val: 1.9285 acc_val: 0.1507 time: 0.0021s\n",
      "Epoch: 0003 loss_train: 1.9356 acc_train: 0.2214 loss_val: 1.9239 acc_val: 0.2130 time: 0.0038s\n",
      "Epoch: 0004 loss_train: 1.9309 acc_train: 0.2714 loss_val: 1.9185 acc_val: 0.2761 time: 0.0034s\n",
      "Epoch: 0005 loss_train: 1.9255 acc_train: 0.3143 loss_val: 1.9129 acc_val: 0.3057 time: 0.0034s\n",
      "Epoch: 0006 loss_train: 1.9174 acc_train: 0.3500 loss_val: 1.9081 acc_val: 0.3154 time: 0.0027s\n",
      "Epoch: 0007 loss_train: 1.9126 acc_train: 0.3286 loss_val: 1.9020 acc_val: 0.3205 time: 0.0026s\n",
      "Epoch: 0008 loss_train: 1.9062 acc_train: 0.3071 loss_val: 1.8961 acc_val: 0.3326 time: 0.0025s\n",
      "Epoch: 0009 loss_train: 1.9001 acc_train: 0.3214 loss_val: 1.8877 acc_val: 0.3415 time: 0.0024s\n",
      "Epoch: 0010 loss_train: 1.8918 acc_train: 0.2857 loss_val: 1.8832 acc_val: 0.3290 time: 0.0025s\n",
      "Epoch: 0011 loss_train: 1.8843 acc_train: 0.3429 loss_val: 1.8758 acc_val: 0.3376 time: 0.0027s\n",
      "Epoch: 0012 loss_train: 1.8780 acc_train: 0.3214 loss_val: 1.8697 acc_val: 0.3544 time: 0.0024s\n",
      "Epoch: 0013 loss_train: 1.8695 acc_train: 0.3000 loss_val: 1.8683 acc_val: 0.3470 time: 0.0031s\n",
      "Epoch: 0014 loss_train: 1.8610 acc_train: 0.3214 loss_val: 1.8591 acc_val: 0.3446 time: 0.0031s\n",
      "Epoch: 0015 loss_train: 1.8535 acc_train: 0.3214 loss_val: 1.8511 acc_val: 0.3415 time: 0.0034s\n",
      "Epoch: 0016 loss_train: 1.8470 acc_train: 0.3500 loss_val: 1.8511 acc_val: 0.3158 time: 0.0036s\n",
      "Epoch: 0017 loss_train: 1.8371 acc_train: 0.3857 loss_val: 1.8404 acc_val: 0.3302 time: 0.0033s\n",
      "Epoch: 0018 loss_train: 1.8334 acc_train: 0.3286 loss_val: 1.8362 acc_val: 0.3275 time: 0.0028s\n",
      "Epoch: 0019 loss_train: 1.8198 acc_train: 0.3143 loss_val: 1.8347 acc_val: 0.3329 time: 0.0028s\n",
      "Epoch: 0020 loss_train: 1.8069 acc_train: 0.3214 loss_val: 1.8277 acc_val: 0.3279 time: 0.0033s\n",
      "Epoch: 0021 loss_train: 1.8139 acc_train: 0.3643 loss_val: 1.8208 acc_val: 0.3306 time: 0.0041s\n",
      "Epoch: 0022 loss_train: 1.7859 acc_train: 0.3643 loss_val: 1.8146 acc_val: 0.3493 time: 0.0030s\n",
      "Epoch: 0023 loss_train: 1.8010 acc_train: 0.3143 loss_val: 1.8026 acc_val: 0.3509 time: 0.0028s\n",
      "Epoch: 0024 loss_train: 1.7729 acc_train: 0.3286 loss_val: 1.8039 acc_val: 0.3614 time: 0.0027s\n",
      "Epoch: 0025 loss_train: 1.7743 acc_train: 0.3500 loss_val: 1.8000 acc_val: 0.3466 time: 0.0030s\n",
      "Epoch: 0026 loss_train: 1.7625 acc_train: 0.3714 loss_val: 1.7878 acc_val: 0.3485 time: 0.0027s\n",
      "Epoch: 0027 loss_train: 1.7447 acc_train: 0.3643 loss_val: 1.7889 acc_val: 0.3376 time: 0.0028s\n",
      "Epoch: 0028 loss_train: 1.7385 acc_train: 0.3571 loss_val: 1.7861 acc_val: 0.3727 time: 0.0034s\n",
      "Epoch: 0029 loss_train: 1.7324 acc_train: 0.3714 loss_val: 1.7709 acc_val: 0.3559 time: 0.0032s\n",
      "Epoch: 0030 loss_train: 1.7256 acc_train: 0.3571 loss_val: 1.7583 acc_val: 0.3777 time: 0.0035s\n",
      "Epoch: 0031 loss_train: 1.6981 acc_train: 0.3643 loss_val: 1.7649 acc_val: 0.3653 time: 0.0030s\n",
      "Epoch: 0032 loss_train: 1.6959 acc_train: 0.4071 loss_val: 1.7530 acc_val: 0.4046 time: 0.0027s\n",
      "Epoch: 0033 loss_train: 1.6843 acc_train: 0.3857 loss_val: 1.7439 acc_val: 0.4190 time: 0.0034s\n",
      "Epoch: 0034 loss_train: 1.6682 acc_train: 0.4643 loss_val: 1.7420 acc_val: 0.4120 time: 0.0032s\n",
      "Epoch: 0035 loss_train: 1.6670 acc_train: 0.4571 loss_val: 1.7252 acc_val: 0.4357 time: 0.0033s\n",
      "Epoch: 0036 loss_train: 1.6337 acc_train: 0.4429 loss_val: 1.7296 acc_val: 0.4490 time: 0.0028s\n",
      "Epoch: 0037 loss_train: 1.6281 acc_train: 0.5071 loss_val: 1.7225 acc_val: 0.4023 time: 0.0028s\n",
      "Epoch: 0038 loss_train: 1.6190 acc_train: 0.5286 loss_val: 1.6994 acc_val: 0.4330 time: 0.0025s\n",
      "Epoch: 0039 loss_train: 1.6091 acc_train: 0.5214 loss_val: 1.6835 acc_val: 0.4591 time: 0.0038s\n",
      "Epoch: 0040 loss_train: 1.5932 acc_train: 0.5286 loss_val: 1.6775 acc_val: 0.4638 time: 0.0035s\n",
      "Epoch: 0041 loss_train: 1.5600 acc_train: 0.5286 loss_val: 1.6807 acc_val: 0.4447 time: 0.0032s\n",
      "Epoch: 0042 loss_train: 1.5638 acc_train: 0.5714 loss_val: 1.6654 acc_val: 0.4681 time: 0.0028s\n",
      "Epoch: 0043 loss_train: 1.5372 acc_train: 0.5714 loss_val: 1.6543 acc_val: 0.4782 time: 0.0032s\n",
      "Epoch: 0044 loss_train: 1.5271 acc_train: 0.5714 loss_val: 1.6392 acc_val: 0.4871 time: 0.0037s\n",
      "Epoch: 0045 loss_train: 1.4688 acc_train: 0.6429 loss_val: 1.6486 acc_val: 0.4646 time: 0.0037s\n",
      "Epoch: 0046 loss_train: 1.4951 acc_train: 0.5714 loss_val: 1.6294 acc_val: 0.4883 time: 0.0031s\n",
      "Epoch: 0047 loss_train: 1.5018 acc_train: 0.5714 loss_val: 1.6197 acc_val: 0.4942 time: 0.0026s\n",
      "Epoch: 0048 loss_train: 1.4473 acc_train: 0.6714 loss_val: 1.6223 acc_val: 0.4840 time: 0.0032s\n",
      "Epoch: 0049 loss_train: 1.4533 acc_train: 0.6429 loss_val: 1.5975 acc_val: 0.5202 time: 0.0033s\n",
      "Epoch: 0050 loss_train: 1.4180 acc_train: 0.6500 loss_val: 1.5853 acc_val: 0.5234 time: 0.0041s\n",
      "Epoch: 0051 loss_train: 1.4111 acc_train: 0.6786 loss_val: 1.5778 acc_val: 0.5187 time: 0.0031s\n",
      "Epoch: 0052 loss_train: 1.3643 acc_train: 0.7071 loss_val: 1.5657 acc_val: 0.5249 time: 0.0026s\n",
      "Epoch: 0053 loss_train: 1.3582 acc_train: 0.7071 loss_val: 1.5643 acc_val: 0.5471 time: 0.0026s\n",
      "Epoch: 0054 loss_train: 1.3302 acc_train: 0.7500 loss_val: 1.5447 acc_val: 0.5654 time: 0.0027s\n",
      "Epoch: 0055 loss_train: 1.3387 acc_train: 0.7214 loss_val: 1.5552 acc_val: 0.5460 time: 0.0026s\n",
      "Epoch: 0056 loss_train: 1.3224 acc_train: 0.7071 loss_val: 1.5317 acc_val: 0.5483 time: 0.0027s\n",
      "Epoch: 0057 loss_train: 1.2699 acc_train: 0.7786 loss_val: 1.5282 acc_val: 0.5611 time: 0.0033s\n",
      "Epoch: 0058 loss_train: 1.2734 acc_train: 0.7571 loss_val: 1.5136 acc_val: 0.5674 time: 0.0035s\n",
      "Epoch: 0059 loss_train: 1.2750 acc_train: 0.7357 loss_val: 1.5086 acc_val: 0.5798 time: 0.0032s\n",
      "Epoch: 0060 loss_train: 1.2447 acc_train: 0.7571 loss_val: 1.5076 acc_val: 0.5681 time: 0.0029s\n",
      "Epoch: 0061 loss_train: 1.2734 acc_train: 0.7571 loss_val: 1.4801 acc_val: 0.5841 time: 0.0027s\n",
      "Epoch: 0062 loss_train: 1.2362 acc_train: 0.7214 loss_val: 1.4685 acc_val: 0.5841 time: 0.0027s\n",
      "Epoch: 0063 loss_train: 1.1943 acc_train: 0.7643 loss_val: 1.4516 acc_val: 0.5849 time: 0.0034s\n",
      "Epoch: 0064 loss_train: 1.1784 acc_train: 0.7500 loss_val: 1.4448 acc_val: 0.6009 time: 0.0041s\n",
      "Epoch: 0065 loss_train: 1.1553 acc_train: 0.7786 loss_val: 1.4461 acc_val: 0.5771 time: 0.0033s\n",
      "Epoch: 0066 loss_train: 1.1501 acc_train: 0.7929 loss_val: 1.4287 acc_val: 0.5997 time: 0.0025s\n",
      "Epoch: 0067 loss_train: 1.1066 acc_train: 0.8000 loss_val: 1.4293 acc_val: 0.5923 time: 0.0026s\n",
      "Epoch: 0068 loss_train: 1.1186 acc_train: 0.8143 loss_val: 1.4204 acc_val: 0.5954 time: 0.0028s\n",
      "Epoch: 0069 loss_train: 1.1132 acc_train: 0.8429 loss_val: 1.3782 acc_val: 0.6386 time: 0.0033s\n",
      "Epoch: 0070 loss_train: 1.1411 acc_train: 0.7929 loss_val: 1.3836 acc_val: 0.6118 time: 0.0035s\n",
      "Epoch: 0071 loss_train: 1.0739 acc_train: 0.8357 loss_val: 1.3824 acc_val: 0.6188 time: 0.0025s\n",
      "Epoch: 0072 loss_train: 1.0491 acc_train: 0.8071 loss_val: 1.3382 acc_val: 0.6394 time: 0.0025s\n",
      "Epoch: 0073 loss_train: 1.0968 acc_train: 0.7786 loss_val: 1.3514 acc_val: 0.6266 time: 0.0026s\n",
      "Epoch: 0074 loss_train: 1.0699 acc_train: 0.8429 loss_val: 1.3393 acc_val: 0.6312 time: 0.0025s\n",
      "Epoch: 0075 loss_train: 1.0787 acc_train: 0.7429 loss_val: 1.3326 acc_val: 0.6340 time: 0.0027s\n",
      "Epoch: 0076 loss_train: 0.9987 acc_train: 0.8643 loss_val: 1.3161 acc_val: 0.6285 time: 0.0021s\n",
      "Epoch: 0077 loss_train: 1.0073 acc_train: 0.8714 loss_val: 1.3234 acc_val: 0.6320 time: 0.0035s\n",
      "Epoch: 0078 loss_train: 1.0382 acc_train: 0.8000 loss_val: 1.3210 acc_val: 0.6488 time: 0.0036s\n",
      "Epoch: 0079 loss_train: 1.0145 acc_train: 0.8286 loss_val: 1.3348 acc_val: 0.6129 time: 0.0033s\n",
      "Epoch: 0080 loss_train: 1.0057 acc_train: 0.8286 loss_val: 1.2935 acc_val: 0.6488 time: 0.0028s\n",
      "Epoch: 0081 loss_train: 0.9517 acc_train: 0.8286 loss_val: 1.2734 acc_val: 0.6421 time: 0.0025s\n",
      "Epoch: 0082 loss_train: 0.9469 acc_train: 0.8286 loss_val: 1.2847 acc_val: 0.6452 time: 0.0024s\n",
      "Epoch: 0083 loss_train: 0.9381 acc_train: 0.8929 loss_val: 1.2867 acc_val: 0.6421 time: 0.0025s\n",
      "Epoch: 0084 loss_train: 0.8802 acc_train: 0.8500 loss_val: 1.2646 acc_val: 0.6616 time: 0.0025s\n",
      "Epoch: 0085 loss_train: 0.9128 acc_train: 0.8857 loss_val: 1.2712 acc_val: 0.6340 time: 0.0024s\n",
      "Epoch: 0086 loss_train: 0.9309 acc_train: 0.8786 loss_val: 1.2432 acc_val: 0.6577 time: 0.0025s\n",
      "Epoch: 0087 loss_train: 0.8687 acc_train: 0.8929 loss_val: 1.2385 acc_val: 0.6484 time: 0.0025s\n",
      "Epoch: 0088 loss_train: 0.8990 acc_train: 0.8571 loss_val: 1.2641 acc_val: 0.6519 time: 0.0027s\n",
      "Epoch: 0089 loss_train: 0.8969 acc_train: 0.8714 loss_val: 1.2420 acc_val: 0.6499 time: 0.0024s\n",
      "Epoch: 0090 loss_train: 0.8969 acc_train: 0.8357 loss_val: 1.2217 acc_val: 0.6667 time: 0.0025s\n",
      "Epoch: 0091 loss_train: 0.8162 acc_train: 0.8929 loss_val: 1.2297 acc_val: 0.6608 time: 0.0026s\n",
      "Epoch: 0092 loss_train: 0.9124 acc_train: 0.8429 loss_val: 1.2022 acc_val: 0.6624 time: 0.0033s\n",
      "Epoch: 0093 loss_train: 0.8378 acc_train: 0.8571 loss_val: 1.1941 acc_val: 0.6671 time: 0.0034s\n",
      "Epoch: 0094 loss_train: 0.8411 acc_train: 0.8429 loss_val: 1.1969 acc_val: 0.6667 time: 0.0035s\n",
      "Epoch: 0095 loss_train: 0.8258 acc_train: 0.8786 loss_val: 1.1897 acc_val: 0.6530 time: 0.0029s\n",
      "Epoch: 0096 loss_train: 0.7872 acc_train: 0.9143 loss_val: 1.1887 acc_val: 0.6589 time: 0.0025s\n",
      "Epoch: 0097 loss_train: 0.7441 acc_train: 0.8857 loss_val: 1.1821 acc_val: 0.6581 time: 0.0027s\n",
      "Epoch: 0098 loss_train: 0.8019 acc_train: 0.8643 loss_val: 1.1773 acc_val: 0.6651 time: 0.0024s\n",
      "Epoch: 0099 loss_train: 0.7884 acc_train: 0.8643 loss_val: 1.1690 acc_val: 0.6682 time: 0.0028s\n",
      "Epoch: 0100 loss_train: 0.7571 acc_train: 0.8786 loss_val: 1.1729 acc_val: 0.6565 time: 0.0027s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3120s\n",
      "Test set results: loss= 1.1568 accuracy= 0.6760\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args[\"epochs\"]):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# evaluating\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZF3eM6DhHfE_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCFwzVLmPXnH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKHEyXp1EVdo"
   },
   "source": [
    "# Graph Attention Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lx15HdotKnt_"
   },
   "source": [
    "## Graph attention layer (Your task)\n",
    "A GAT is made up of multiple such layers. In this section, you will implement a single graph attention layer. Similar to the `GraphConvolution()`, this `GraphAttentionLayer()` module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$. However, instead of weighing each neighbouring node based on the adjacency matrix, we will use self attention to learn the relative importance of each neighbouring node. Recall from HW4 where you are asked to write out the equation for single headed attention, here we will implement multi-headed attention, which involves the following steps: \n",
    "\n",
    "\n",
    "### The initial transformation\n",
    "In GCN above, you have completed similar transformation. But here, we need to define a weight matrix and perform this transformation for each head: $\\overrightarrow{s^k_i} = \\mathbf{W}^k \\overrightarrow{h_i}$. We will perform a single linear transformation and then split it up for each head later. Note the input $\\overrightarrow{h}$ has shape `[n_nodes, in_features]` and $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads * n_hidden]`. Remember to reshape $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads, n_hidden]` for later uses. Note: set `bias=False` for this linear transformation. \n",
    "\n",
    "### attention score\n",
    "We calculate these for each head $k$. Here for simplicity of the notation, we omit $k$ in the following equations. The attention scores are defined as the follows: \n",
    "$$e_{ij} = a(\\mathbf{W} \\overrightarrow{h_i}, \\mathbf{W} \\overrightarrow{h_j}) =a(\\overrightarrow{s_i}, \\overrightarrow{s_j})$$, \n",
    "where $e_{ij}$ is the attention score (importance) of node $j$ to node $i$.\n",
    "We will have to calculate this for each head. $a$ is the attention mechanism, that calculates the attention score. The paper concatenates $\\overrightarrow{s_i}$, $\\overrightarrow{s_j}$ and does a linear transformation with a weight vector $\\mathbf{a} \\in \\mathbb{R}^{2 F'}$ followed by a $\\text{LeakyReLU}$. $$e_{ij} = \\text{LeakyReLU} \\Big(\n",
    "\\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$$\n",
    "\n",
    "#### How to vectorize this? Some hints: \n",
    "1. `tensor.repeat()` gives you $\\{\\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, ...\\}$.\n",
    "\n",
    "2. `tensor.repeat_interleave()` gives you\n",
    "$\\{\\overrightarrow{s_1}, \\overrightarrow{s_1}, \\dots, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_2}, ...\\}$.\n",
    "\n",
    "3. concatenate to get $\\Big[\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j} \\Big]$ for all pairs of $i, j$. Reshape $\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}$ has shape of `[n_nodes, n_nodes, n_heads, 2 * n_hidden]`\n",
    "\n",
    "4. apply the attention layer and non-linear activation function to get $e_{ij} = \\text{LeakyReLU} \\Big( \\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$, where $\\mathbf{a}^\\top$ is a single linear transformation that maps from dimension `n_hidden * 2` to `1`. Note: set the `bias=False` for this linear transformation. $\\mathbf{e}$ is of shape `[n_nodes, n_nodes, n_heads, 1]`. Remove the last dimension `1` using `squeeze()`. \n",
    "\n",
    "\n",
    "#### Perform softmax \n",
    "First, we need to mask $e_{ij}$ based on adjacency matrix. We only need to sum over the neighbouring nodes for the attention calculation. Set the elements in $e_{ij}$ to $- \\infty$ if there is no edge from $i$ to $j$ for the softmax calculation. We need to do this for all heads and the adjacency matrix is the same for each head. Use `tensor.masked_fill()` to mask $e_{ij}$ based on adjacency matrix for all heads. Hint: reshape the adjacency matrix to `[n_nodes, n_nodes, 1]` using `unsqueeze()`. \n",
    "Now we are ready to normalize attention scores (or coefficients) $$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) =  \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}$$\n",
    "\n",
    "#### Apply dropout\n",
    "Apply the dropout layer. (this step is easy)\n",
    "\n",
    "#### Calculate final output for each head\n",
    "$$\\overrightarrow{h'^k_i} = \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\overrightarrow{s^k_j}$$\n",
    "\n",
    "\n",
    "#### Concat or Mean\n",
    "Finally we concateneate the transformed features: $\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$. In the code, we only need to reshape the tensor to shape of `[n_nodes, n_heads * n_hidden]`. Note that if it is the final layer, then it doesn't make sense to do concatenation anymore. Instead, we sum over the `n_heads` dimension: $\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "wVu7rcOuAUZz"
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
    "                 is_concat: bool = True,\n",
    "                 dropout: float = 0.6,\n",
    "                 alpha: float = 0.2):\n",
    "        \"\"\"\n",
    "        in_features: F, the number of input features per node\n",
    "        out_features: F', the number of output features per node\n",
    "        n_heads: K, the number of attention heads\n",
    "        is_concat: whether the multi-head results should be concatenated or averaged\n",
    "        dropout: the dropout probability\n",
    "        alpha: the negative slope for leaky relu activation\n",
    "        \"\"\"\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        # TODO: initialize the following modules: \n",
    "        # (1) self.W: Linear layer that transform the input feature before self attention. \n",
    "        # You should NOT use for loops for the multiheaded implementation (set bias = Flase)\n",
    "        # (2) self.attention: Linear layer that compute the attention score (set bias = Flase)\n",
    "        # (3) self.activation: Activation function (LeakyReLU whith negative_slope=alpha)\n",
    "        # (4) self.softmax: Softmax function (what's the dim to compute the summation?)\n",
    "        # (5) self.dropout_layer: Dropout function(with ratio=dropout)\n",
    "        ################ your code here ########################\n",
    "\n",
    "        self.W = nn.Linear(in_features, n_heads*self.n_hidden, bias=False)\n",
    "        self.attention = nn.Linear(self.n_hidden*2, 1, bias=False)\n",
    "        self.activation = nn.LeakyReLU(alpha)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "        ########################################################\n",
    "\n",
    "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        # Number of nodes\n",
    "        n_nodes = h.shape[0]\n",
    "        \n",
    "        # TODO: \n",
    "        # (1) calculate s = Wh and reshape it to [n_nodes, n_heads, n_hidden] \n",
    "        #     (you can use tensor.view() function)\n",
    "        # (2) get [s_i || s_j] using tensor.repeat(), repeat_interleave(), torch.cat(), tensor.view()  \n",
    "        # (3) apply the attention layer \n",
    "        # (4) apply the activation layer (you will get the attention score e)\n",
    "        # (5) remove the last dimension 1 use tensor.squeeze()\n",
    "        # (6) mask the attention score with the adjacency matrix (if there's no edge, assign it to -inf)\n",
    "        #     note: check the dimensions of e and your adjacency matrix. Y\n",
    "        #.    ou may need to use the function unsqueeze()\n",
    "        # (7) apply softmax \n",
    "        # (8) apply dropout_layer \n",
    "        ############## Your code here #########################################\n",
    "        #(1)\n",
    "        s = self.W(h)\n",
    "        s = s.view((n_nodes, self.n_heads, self.n_hidden)) #torch.Size([2708, 8, 2])\n",
    "\n",
    "        #(2)\n",
    "        s_i = s.repeat((n_nodes,1,1)) #torch.Size([7333264, 8, 2])\n",
    "        s_j = s.repeat_interleave(n_nodes, dim=0) #torch.Size([7333264, 8, 2])\n",
    "\n",
    "        cat_s = torch.cat((s_i, s_j), dim=-1)\n",
    "        cat_s = cat_s.view((n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)) #torch.Size([2708, 2708, 8, 4])\n",
    "\n",
    "        #(3) attention layer\n",
    "        a = self.attention(cat_s) #torch.Size([2708, 2708, 8, 1])\n",
    "\n",
    "        #(4) activation layer and (5)\n",
    "        e = self.activation(a).squeeze(-1) #torch.Size([2708, 2708, 8])\n",
    "        \n",
    "        #(6)\n",
    "        mask = -np.inf*torch.ones_like(e) #torch.Size([2708, 2708, 8])\n",
    "        masked_e = torch.where(adj_mat.unsqueeze(-1) > 0, e, mask) #torch.Size([2708, 2708, 8])\n",
    "        \n",
    "        # (7) softmax and (8)dropout\n",
    "        masked_e = self.softmax(masked_e)\n",
    "        masked_e = self.dropout(masked_e) #torch.Size([2708, 2708, 8])\n",
    "\n",
    "        #######################################################################\n",
    "\n",
    "        # Summation \n",
    "        h_prime = torch.einsum('ijh,jhf->ihf', masked_e, s) #[n_nodes, n_heads, n_hidden] #torch.Size([2708, 8, 2])\n",
    "\n",
    "\n",
    "        # TODO: Concat or Mean\n",
    "        # Concatenate the heads\n",
    "        if self.is_concat:\n",
    "            ############## Your code here #########################################\n",
    "            #In the code, we only need to reshape the tensor to shape of [n_nodes, n_heads * n_hidden]\n",
    "            return h_prime.reshape(n_nodes, self.n_heads*self.n_hidden)\n",
    "\n",
    "            #######################################################################\n",
    "        # Take the mean of the heads (for the last layer)\n",
    "        else:\n",
    "            ############## Your code here #########################################\n",
    "            #instead, we sum over the n_heads dimension:\n",
    "            return torch.sum(h_prime, dim=1)\n",
    "\n",
    "            #######################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOSk_ZShi2nR"
   },
   "source": [
    "## Define GAT network\n",
    "it's really similar to how we defined GCN. We followed the paper to use two attention layers and ELU() activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "jKNbUtPVi1Vs"
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "\n",
    "    def __init__(self, nfeat: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float, alpha: float):\n",
    "        \"\"\"\n",
    "        in_features: the number of features per node\n",
    "        n_hidden: the number of features in the first graph attention layer\n",
    "        n_classes: the number of classes\n",
    "        n_heads: the number of heads in the graph attention layers\n",
    "        dropout: the dropout probability\n",
    "        alpha: the negative input slope for leaky ReLU of the attention layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # First graph attention layer where we concatenate the heads\n",
    "        self.gc1 = GraphAttentionLayer(nfeat, n_hidden, n_heads, is_concat=True, dropout=dropout, alpha=alpha)\n",
    "        self.gc2 = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout, alpha=alpha)\n",
    "        self.activation = nn.ELU()  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: the features vectors\n",
    "        adj_mat: the adjacency matrix\n",
    "        \"\"\"\n",
    "        x = self.dropout(x)\n",
    "        x = self.gc1(x, adj_mat)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gc2(x, adj_mat)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtRQ3Ced7RAw"
   },
   "source": [
    "## training GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "b7D5mYXC6zTG"
   },
   "outputs": [],
   "source": [
    "args = {\"training_samples\": 140,\n",
    "        \"epochs\": 100,\n",
    "        \"lr\": 0.01,\n",
    "        \"weight_decay\": 5e-4,\n",
    "        \"hidden\": 16,\n",
    "        \"dropout\": 0.5,\n",
    "        \"bias\": True, \n",
    "        \"alpha\": 0.2,\n",
    "        \"n_heads\": 8\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "7MYaK98hDy7u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "model = GAT(nfeat=features.shape[1],\n",
    "            n_hidden=args[\"hidden\"],\n",
    "            n_classes=labels.max().item() + 1,\n",
    "            dropout=args[\"dropout\"],\n",
    "            alpha=args[\"alpha\"],\n",
    "            n_heads=args[\"n_heads\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
    "\n",
    "adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n",
    "adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "E9FcfXwMDzEt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9461 acc_train: 0.1857 loss_val: 1.9425 acc_val: 0.5592 time: 0.1139s\n",
      "Epoch: 0002 loss_train: 1.9415 acc_train: 0.4786 loss_val: 1.9390 acc_val: 0.5152 time: 0.1132s\n",
      "Epoch: 0003 loss_train: 1.9371 acc_train: 0.5929 loss_val: 1.9349 acc_val: 0.4770 time: 0.1081s\n",
      "Epoch: 0004 loss_train: 1.9324 acc_train: 0.4929 loss_val: 1.9306 acc_val: 0.4642 time: 0.1063s\n",
      "Epoch: 0005 loss_train: 1.9276 acc_train: 0.4929 loss_val: 1.9260 acc_val: 0.4564 time: 0.1073s\n",
      "Epoch: 0006 loss_train: 1.9195 acc_train: 0.5286 loss_val: 1.9209 acc_val: 0.4396 time: 0.1063s\n",
      "Epoch: 0007 loss_train: 1.9147 acc_train: 0.5214 loss_val: 1.9155 acc_val: 0.4326 time: 0.1071s\n",
      "Epoch: 0008 loss_train: 1.9060 acc_train: 0.5214 loss_val: 1.9096 acc_val: 0.4256 time: 0.1070s\n",
      "Epoch: 0009 loss_train: 1.8981 acc_train: 0.4786 loss_val: 1.9034 acc_val: 0.4202 time: 0.1068s\n",
      "Epoch: 0010 loss_train: 1.8994 acc_train: 0.4786 loss_val: 1.8970 acc_val: 0.4171 time: 0.1066s\n",
      "Epoch: 0011 loss_train: 1.8836 acc_train: 0.4857 loss_val: 1.8902 acc_val: 0.4151 time: 0.1062s\n",
      "Epoch: 0012 loss_train: 1.8746 acc_train: 0.4286 loss_val: 1.8829 acc_val: 0.4116 time: 0.1072s\n",
      "Epoch: 0013 loss_train: 1.8568 acc_train: 0.4429 loss_val: 1.8752 acc_val: 0.4073 time: 0.1064s\n",
      "Epoch: 0014 loss_train: 1.8562 acc_train: 0.5000 loss_val: 1.8670 acc_val: 0.4062 time: 0.1066s\n",
      "Epoch: 0015 loss_train: 1.8510 acc_train: 0.4357 loss_val: 1.8586 acc_val: 0.4050 time: 0.1072s\n",
      "Epoch: 0016 loss_train: 1.8497 acc_train: 0.4571 loss_val: 1.8499 acc_val: 0.4065 time: 0.1070s\n",
      "Epoch: 0017 loss_train: 1.8031 acc_train: 0.5214 loss_val: 1.8407 acc_val: 0.4069 time: 0.1068s\n",
      "Epoch: 0018 loss_train: 1.8000 acc_train: 0.4714 loss_val: 1.8310 acc_val: 0.4073 time: 0.1063s\n",
      "Epoch: 0019 loss_train: 1.8079 acc_train: 0.4357 loss_val: 1.8211 acc_val: 0.4116 time: 0.1063s\n",
      "Epoch: 0020 loss_train: 1.7910 acc_train: 0.4357 loss_val: 1.8109 acc_val: 0.4147 time: 0.1062s\n",
      "Epoch: 0021 loss_train: 1.7701 acc_train: 0.4786 loss_val: 1.8002 acc_val: 0.4155 time: 0.1069s\n",
      "Epoch: 0022 loss_train: 1.7716 acc_train: 0.4071 loss_val: 1.7893 acc_val: 0.4186 time: 0.1063s\n",
      "Epoch: 0023 loss_train: 1.7693 acc_train: 0.4786 loss_val: 1.7783 acc_val: 0.4221 time: 0.1065s\n",
      "Epoch: 0024 loss_train: 1.7587 acc_train: 0.5000 loss_val: 1.7672 acc_val: 0.4280 time: 0.1063s\n",
      "Epoch: 0025 loss_train: 1.7332 acc_train: 0.4571 loss_val: 1.7558 acc_val: 0.4330 time: 0.1069s\n",
      "Epoch: 0026 loss_train: 1.7305 acc_train: 0.5357 loss_val: 1.7443 acc_val: 0.4439 time: 0.1064s\n",
      "Epoch: 0027 loss_train: 1.7022 acc_train: 0.5214 loss_val: 1.7326 acc_val: 0.4568 time: 0.1063s\n",
      "Epoch: 0028 loss_train: 1.6914 acc_train: 0.5214 loss_val: 1.7208 acc_val: 0.4700 time: 0.1072s\n",
      "Epoch: 0029 loss_train: 1.6846 acc_train: 0.5500 loss_val: 1.7088 acc_val: 0.4790 time: 0.1063s\n",
      "Epoch: 0030 loss_train: 1.6908 acc_train: 0.4429 loss_val: 1.6969 acc_val: 0.4907 time: 0.1068s\n",
      "Epoch: 0031 loss_train: 1.6642 acc_train: 0.5714 loss_val: 1.6849 acc_val: 0.5019 time: 0.1062s\n",
      "Epoch: 0032 loss_train: 1.6300 acc_train: 0.5071 loss_val: 1.6727 acc_val: 0.5125 time: 0.1063s\n",
      "Epoch: 0033 loss_train: 1.5938 acc_train: 0.5786 loss_val: 1.6605 acc_val: 0.5253 time: 0.1062s\n",
      "Epoch: 0034 loss_train: 1.6309 acc_train: 0.5714 loss_val: 1.6483 acc_val: 0.5374 time: 0.1063s\n",
      "Epoch: 0035 loss_train: 1.5882 acc_train: 0.6143 loss_val: 1.6361 acc_val: 0.5495 time: 0.1075s\n",
      "Epoch: 0036 loss_train: 1.5111 acc_train: 0.6286 loss_val: 1.6235 acc_val: 0.5631 time: 0.1062s\n",
      "Epoch: 0037 loss_train: 1.5380 acc_train: 0.6500 loss_val: 1.6108 acc_val: 0.5775 time: 0.1063s\n",
      "Epoch: 0038 loss_train: 1.5432 acc_train: 0.6643 loss_val: 1.5979 acc_val: 0.5907 time: 0.1069s\n",
      "Epoch: 0039 loss_train: 1.5507 acc_train: 0.6571 loss_val: 1.5850 acc_val: 0.6016 time: 0.1068s\n",
      "Epoch: 0040 loss_train: 1.4877 acc_train: 0.6929 loss_val: 1.5722 acc_val: 0.6114 time: 0.1062s\n",
      "Epoch: 0041 loss_train: 1.5235 acc_train: 0.6143 loss_val: 1.5595 acc_val: 0.6234 time: 0.1063s\n",
      "Epoch: 0042 loss_train: 1.4905 acc_train: 0.6786 loss_val: 1.5471 acc_val: 0.6343 time: 0.1070s\n",
      "Epoch: 0043 loss_train: 1.4671 acc_train: 0.7071 loss_val: 1.5346 acc_val: 0.6480 time: 0.1066s\n",
      "Epoch: 0044 loss_train: 1.4375 acc_train: 0.6714 loss_val: 1.5224 acc_val: 0.6573 time: 0.1070s\n",
      "Epoch: 0045 loss_train: 1.4341 acc_train: 0.7786 loss_val: 1.5103 acc_val: 0.6706 time: 0.1062s\n",
      "Epoch: 0046 loss_train: 1.4530 acc_train: 0.7000 loss_val: 1.4982 acc_val: 0.6834 time: 0.1069s\n",
      "Epoch: 0047 loss_train: 1.3843 acc_train: 0.7000 loss_val: 1.4865 acc_val: 0.6963 time: 0.1069s\n",
      "Epoch: 0048 loss_train: 1.3369 acc_train: 0.7500 loss_val: 1.4748 acc_val: 0.7033 time: 0.1070s\n",
      "Epoch: 0049 loss_train: 1.3599 acc_train: 0.7571 loss_val: 1.4633 acc_val: 0.7091 time: 0.1069s\n",
      "Epoch: 0050 loss_train: 1.3404 acc_train: 0.7286 loss_val: 1.4519 acc_val: 0.7204 time: 0.1064s\n",
      "Epoch: 0051 loss_train: 1.3353 acc_train: 0.7143 loss_val: 1.4406 acc_val: 0.7231 time: 0.1064s\n",
      "Epoch: 0052 loss_train: 1.3127 acc_train: 0.7143 loss_val: 1.4292 acc_val: 0.7301 time: 0.1064s\n",
      "Epoch: 0053 loss_train: 1.3142 acc_train: 0.7571 loss_val: 1.4179 acc_val: 0.7352 time: 0.1063s\n",
      "Epoch: 0054 loss_train: 1.2647 acc_train: 0.7857 loss_val: 1.4067 acc_val: 0.7418 time: 0.1070s\n",
      "Epoch: 0055 loss_train: 1.3229 acc_train: 0.7286 loss_val: 1.3957 acc_val: 0.7473 time: 0.1062s\n",
      "Epoch: 0056 loss_train: 1.2483 acc_train: 0.7857 loss_val: 1.3846 acc_val: 0.7535 time: 0.1075s\n",
      "Epoch: 0057 loss_train: 1.2284 acc_train: 0.8071 loss_val: 1.3736 acc_val: 0.7562 time: 0.1062s\n",
      "Epoch: 0058 loss_train: 1.3117 acc_train: 0.8214 loss_val: 1.3626 acc_val: 0.7609 time: 0.1063s\n",
      "Epoch: 0059 loss_train: 1.2784 acc_train: 0.7500 loss_val: 1.3522 acc_val: 0.7648 time: 0.1063s\n",
      "Epoch: 0060 loss_train: 1.3087 acc_train: 0.7500 loss_val: 1.3420 acc_val: 0.7706 time: 0.1073s\n",
      "Epoch: 0061 loss_train: 1.2312 acc_train: 0.7500 loss_val: 1.3322 acc_val: 0.7749 time: 0.1076s\n",
      "Epoch: 0062 loss_train: 1.1903 acc_train: 0.7929 loss_val: 1.3224 acc_val: 0.7804 time: 0.1069s\n",
      "Epoch: 0063 loss_train: 1.1973 acc_train: 0.7857 loss_val: 1.3124 acc_val: 0.7847 time: 0.1062s\n",
      "Epoch: 0064 loss_train: 1.1544 acc_train: 0.8143 loss_val: 1.3023 acc_val: 0.7882 time: 0.1064s\n",
      "Epoch: 0065 loss_train: 1.1165 acc_train: 0.8286 loss_val: 1.2921 acc_val: 0.7886 time: 0.1068s\n",
      "Epoch: 0066 loss_train: 1.0978 acc_train: 0.8071 loss_val: 1.2825 acc_val: 0.7917 time: 0.1063s\n",
      "Epoch: 0067 loss_train: 1.1708 acc_train: 0.7929 loss_val: 1.2731 acc_val: 0.7924 time: 0.1067s\n",
      "Epoch: 0068 loss_train: 1.1940 acc_train: 0.7571 loss_val: 1.2639 acc_val: 0.7913 time: 0.1069s\n",
      "Epoch: 0069 loss_train: 1.0739 acc_train: 0.8357 loss_val: 1.2546 acc_val: 0.7967 time: 0.1065s\n",
      "Epoch: 0070 loss_train: 1.1182 acc_train: 0.7857 loss_val: 1.2454 acc_val: 0.7975 time: 0.1069s\n",
      "Epoch: 0071 loss_train: 1.0789 acc_train: 0.8000 loss_val: 1.2359 acc_val: 0.7991 time: 0.1070s\n",
      "Epoch: 0072 loss_train: 1.0684 acc_train: 0.8071 loss_val: 1.2263 acc_val: 0.7995 time: 0.1069s\n",
      "Epoch: 0073 loss_train: 1.1281 acc_train: 0.7571 loss_val: 1.2166 acc_val: 0.7998 time: 0.1063s\n",
      "Epoch: 0074 loss_train: 1.0819 acc_train: 0.8286 loss_val: 1.2068 acc_val: 0.8026 time: 0.1064s\n",
      "Epoch: 0075 loss_train: 0.9659 acc_train: 0.8429 loss_val: 1.1968 acc_val: 0.8030 time: 0.1063s\n",
      "Epoch: 0076 loss_train: 1.0352 acc_train: 0.8357 loss_val: 1.1868 acc_val: 0.8041 time: 0.1064s\n",
      "Epoch: 0077 loss_train: 1.0538 acc_train: 0.8357 loss_val: 1.1769 acc_val: 0.8057 time: 0.1066s\n",
      "Epoch: 0078 loss_train: 1.0164 acc_train: 0.8000 loss_val: 1.1670 acc_val: 0.8061 time: 0.1063s\n",
      "Epoch: 0079 loss_train: 1.0091 acc_train: 0.7786 loss_val: 1.1574 acc_val: 0.8061 time: 0.1063s\n",
      "Epoch: 0080 loss_train: 1.0237 acc_train: 0.7929 loss_val: 1.1480 acc_val: 0.8072 time: 0.1070s\n",
      "Epoch: 0081 loss_train: 1.0372 acc_train: 0.8071 loss_val: 1.1390 acc_val: 0.8072 time: 0.1070s\n",
      "Epoch: 0082 loss_train: 0.9936 acc_train: 0.8357 loss_val: 1.1305 acc_val: 0.8088 time: 0.1071s\n",
      "Epoch: 0083 loss_train: 0.9844 acc_train: 0.7786 loss_val: 1.1222 acc_val: 0.8084 time: 0.1066s\n",
      "Epoch: 0084 loss_train: 0.9709 acc_train: 0.8786 loss_val: 1.1138 acc_val: 0.8092 time: 0.1067s\n",
      "Epoch: 0085 loss_train: 1.0086 acc_train: 0.8000 loss_val: 1.1056 acc_val: 0.8104 time: 0.1067s\n",
      "Epoch: 0086 loss_train: 0.9453 acc_train: 0.8071 loss_val: 1.0976 acc_val: 0.8111 time: 0.1067s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 loss_train: 0.9404 acc_train: 0.8143 loss_val: 1.0895 acc_val: 0.8119 time: 0.1075s\n",
      "Epoch: 0088 loss_train: 0.9325 acc_train: 0.7643 loss_val: 1.0816 acc_val: 0.8123 time: 0.1073s\n",
      "Epoch: 0089 loss_train: 0.8658 acc_train: 0.8571 loss_val: 1.0741 acc_val: 0.8127 time: 0.1073s\n",
      "Epoch: 0090 loss_train: 0.8828 acc_train: 0.8929 loss_val: 1.0664 acc_val: 0.8139 time: 0.1068s\n",
      "Epoch: 0091 loss_train: 0.9551 acc_train: 0.8000 loss_val: 1.0589 acc_val: 0.8135 time: 0.1075s\n",
      "Epoch: 0092 loss_train: 0.8869 acc_train: 0.8357 loss_val: 1.0517 acc_val: 0.8139 time: 0.1074s\n",
      "Epoch: 0093 loss_train: 0.8949 acc_train: 0.8143 loss_val: 1.0444 acc_val: 0.8150 time: 0.1066s\n",
      "Epoch: 0094 loss_train: 0.8864 acc_train: 0.8286 loss_val: 1.0374 acc_val: 0.8178 time: 0.1066s\n",
      "Epoch: 0095 loss_train: 0.8668 acc_train: 0.8143 loss_val: 1.0306 acc_val: 0.8185 time: 0.1069s\n",
      "Epoch: 0096 loss_train: 0.8360 acc_train: 0.8571 loss_val: 1.0236 acc_val: 0.8193 time: 0.1069s\n",
      "Epoch: 0097 loss_train: 0.9084 acc_train: 0.7929 loss_val: 1.0170 acc_val: 0.8197 time: 0.1073s\n",
      "Epoch: 0098 loss_train: 0.8461 acc_train: 0.8643 loss_val: 1.0107 acc_val: 0.8220 time: 0.1072s\n",
      "Epoch: 0099 loss_train: 0.8349 acc_train: 0.8143 loss_val: 1.0045 acc_val: 0.8217 time: 0.1071s\n",
      "Epoch: 0100 loss_train: 0.8310 acc_train: 0.8357 loss_val: 0.9985 acc_val: 0.8224 time: 0.1068s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 10.7039s\n",
      "Test set results: loss= 0.9985 accuracy= 0.8224\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args[\"epochs\"]):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6Ox3fbTG7rc"
   },
   "source": [
    "# Question: (Your task)\n",
    "Compare the evaluation results for Vanilla GCN and GAT. Comment on the discrepancy in their performance (if any) and briefly explain why you think it's the case (in 1-2 sentences). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urJ8Q-neDzHU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZhHh8k4DzJu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmvJ46OfGlf2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "a4_GCN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
